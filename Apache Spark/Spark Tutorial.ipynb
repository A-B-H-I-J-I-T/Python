{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\abhij\\\\anaconda3\\\\lib\\\\site-packages\\\\pyspark'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import findspark to verify if the package is installed\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "#to get rid of all the environment issue in conda use this command to download java version 8: conda install -c cyclus java-jdk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maps and Lambda functions\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"map_lambda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "despacito\n",
      "['despacito', 'nice for what', 'no tears left to cry', 'despacito', 'havana', 'in my feelings', 'nice for what', 'despacito', 'all the stars']\n",
      "['despacito', 'nice for what', 'no tears left to cry', 'despacito', 'havana', 'in my feelings', 'nice for what', 'despacito', 'all the stars']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Despacito',\n",
       " 'Nice for what',\n",
       " 'No tears left to cry',\n",
       " 'Despacito',\n",
       " 'Havana',\n",
       " 'In my feelings',\n",
       " 'Nice for what',\n",
       " 'despacito',\n",
       " 'All the stars']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "log_of_songs = [\n",
    "        \"Despacito\",\n",
    "        \"Nice for what\",\n",
    "        \"No tears left to cry\",\n",
    "        \"Despacito\",\n",
    "        \"Havana\",\n",
    "        \"In my feelings\",\n",
    "        \"Nice for what\",\n",
    "        \"despacito\",\n",
    "        \"All the stars\"\n",
    "]\n",
    "#parallelize the log_of_song to use with spark\n",
    "distributed_song_log = sc.parallelize(log_of_songs)\n",
    "\n",
    "#either define a function to convert the song name to lower case or use a lambda function\n",
    "def convert_song_to_lowercase(song):\n",
    "    return song.lower()\n",
    "\n",
    "print(convert_song_to_lowercase(\"Despacito\"))\n",
    "\n",
    "#mapfunction of the distributed spark object\n",
    "sparkOutput = distributed_song_log.map(convert_song_to_lowercase).collect()\n",
    "print(sparkOutput)\n",
    "sparkOuputlambda = distributed_song_log.map(lambda song:song.lower()).collect()\n",
    "print(sparkOuputlambda)\n",
    "distributed_song_log.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Create SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|language|users_count|\n",
      "+--------+-----------+\n",
      "|    Java|      20000|\n",
      "|  Python|     100000|\n",
      "|   Scala|       3000|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Data\n",
    "data = [(\"Java\", \"20000\"), (\"Python\", \"100000\"), (\"Scala\", \"3000\")]\n",
    "\n",
    "# Columns\n",
    "columns = [\"language\",\"users_count\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data).toDF(*columns)\n",
    "\n",
    "# Print DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "path file:/d:/Git/Python/Apache Spark/Data/csvout.csv already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Git\\Python\\Apache Spark\\Spark Tutorial.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Git/Python/Apache%20Spark/Spark%20Tutorial.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m df\u001b[39m=\u001b[39mspark\u001b[39m.\u001b[39mcreateDataFrame(data,columns)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Git/Python/Apache%20Spark/Spark%20Tutorial.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# write a new parquet file\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Git/Python/Apache%20Spark/Spark%20Tutorial.ipynb#W6sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m df\u001b[39m.\u001b[39;49mwrite\u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39mheader\u001b[39;49m\u001b[39m\"\u001b[39;49m,\u001b[39mTrue\u001b[39;49;00m) \\\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Git/Python/Apache%20Spark/Spark%20Tutorial.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m  \u001b[39m.\u001b[39;49mcsv(\u001b[39m\"\u001b[39;49m\u001b[39mData/csvout.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\abhij\\anaconda3\\lib\\site-packages\\pyspark\\sql\\readwriter.py:1372\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m   1364\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode(mode)\n\u001b[0;32m   1365\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_opts(compression\u001b[39m=\u001b[39mcompression, sep\u001b[39m=\u001b[39msep, quote\u001b[39m=\u001b[39mquote, escape\u001b[39m=\u001b[39mescape, header\u001b[39m=\u001b[39mheader,\n\u001b[0;32m   1366\u001b[0m                nullValue\u001b[39m=\u001b[39mnullValue, escapeQuotes\u001b[39m=\u001b[39mescapeQuotes, quoteAll\u001b[39m=\u001b[39mquoteAll,\n\u001b[0;32m   1367\u001b[0m                dateFormat\u001b[39m=\u001b[39mdateFormat, timestampFormat\u001b[39m=\u001b[39mtimestampFormat,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1370\u001b[0m                charToEscapeQuoteEscaping\u001b[39m=\u001b[39mcharToEscapeQuoteEscaping,\n\u001b[0;32m   1371\u001b[0m                encoding\u001b[39m=\u001b[39mencoding, emptyValue\u001b[39m=\u001b[39memptyValue, lineSep\u001b[39m=\u001b[39mlineSep)\n\u001b[1;32m-> 1372\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49mcsv(path)\n",
      "File \u001b[1;32mc:\\Users\\abhij\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py:1309\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1303\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1304\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1305\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1306\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1308\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1309\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1310\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1312\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1313\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mc:\\Users\\abhij\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    113\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[0;32m    114\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    115\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    116\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    118\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: path file:/d:/Git/Python/Apache Spark/Data/csvout.csv already exists."
     ]
    }
   ],
   "source": [
    "# read and write a parquet file\n",
    "data =[(\"James \",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
    "              (\"Michael \",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "              (\"Robert \",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
    "              (\"Maria \",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "              (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)]\n",
    "columns=[\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
    "df=spark.createDataFrame(data,columns)\n",
    "\n",
    "# write a new parquet file\n",
    "\n",
    "df.write.option(\"header\",True) \\\n",
    " .csv(\"Data/csvout.csv\")\n",
    "\n",
    "\n",
    "#parDF=spark.read.parquet(\"/Data/people.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+----------+---------------+------------------------+-------------------------+\n",
      "|appointment_date|clinic_name|patient_id|practitioner_id|appointment_duration_min|revenues_from_appointment|\n",
      "+----------------+-----------+----------+---------------+------------------------+-------------------------+\n",
      "|      2021-12-30|   clinic_2|     70172|           1030|                      30|                    98.13|\n",
      "|      2021-12-29|   clinic_1|     53371|            756|                      15|                    65.13|\n",
      "|      2021-12-29|   clinic_2|     69545|           1030|                      30|                   105.18|\n",
      "|      2021-12-27|   clinic_1|     43859|            756|                      30|                    58.11|\n",
      "|      2021-12-23|   clinic_1|     69303|            756|                      30|                    119.0|\n",
      "|      2021-12-23|   clinic_1|     44545|            756|                      30|                    58.11|\n",
      "|      2021-12-21|   clinic_1|     68724|            756|                      15|                    50.85|\n",
      "|      2021-12-22|   clinic_1|     68332|            756|                      15|                    41.71|\n",
      "|      2021-12-27|   clinic_2|     66365|           1030|                      45|                   213.59|\n",
      "|      2021-12-20|   clinic_1|     67590|            756|                      15|                    83.87|\n",
      "+----------------+-----------+----------+---------------+------------------------+-------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parDF=spark.read.csv(\"D:/Git/Python/Apache Spark/Data/fake_clinic_data_extract_test.csv\",header=True)\n",
    "parDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version = 3.1.2\n",
      "Hadoop version = 3.2.0\n"
     ]
    }
   ],
   "source": [
    "# spark\n",
    "print(f\"Spark version = {spark.version}\")\n",
    "\n",
    "# hadoop\n",
    "print(f\"Hadoop version = {sc._jvm.org.apache.hadoop.util.VersionInfo.getVersion()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "198dc38c81252feb4faa0d475279f5f6c993dd4570bd3709069413259282040c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
