{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import all the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import psycopg2\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract customer id and contact_email from the table customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#lets create a function to connect to DB\n",
    "\n",
    "def connect_to_postgres():\n",
    "#lets declare all the connection parameters to the postgres DB\n",
    "    db_params = {\n",
    "                \"host\":'localhost',\n",
    "                \"database\":'postgres',\n",
    "                \"user\":'postgres',\n",
    "                \"password\":'Your Server Password'\n",
    "                }\n",
    "    connection = psycopg2.connect(**db_params)\n",
    "    return connection\n",
    "\n",
    "# declare the name of your PostgreSQL table and columns\n",
    "table_name = 'customers'\n",
    "column_name1 = 'id'\n",
    "colum_name2 = 'contact_email'\n",
    "\n",
    "try: \n",
    "    connection = connect_to_postgres()\n",
    "    # Construct the SQL query to select columns from the customer table\n",
    "    query = f\"SELECT {column_name1},{colum_name2} FROM {table_name}\"\n",
    "\n",
    "    # read the data into a DataFrame\n",
    "    df = pd.read_sql_query(query, connection)\n",
    "\n",
    "    # Close the database connection\n",
    "    connection.close()\n",
    "except psycopg2.Error as error:\n",
    "    print(\"Error occured, Details:\", error)\n",
    "except Exception as error:\n",
    "    print(\"Error occured, Details:\", error)\n",
    "finally:\n",
    "    connection.close()\n",
    "\n",
    "#create a copy of the df which will later used to merge after the indutry information is retrieved\n",
    "df_backup = df.copy()\n",
    "\n",
    "#  Display the DataFrame\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract the domain name from email and remove duplicates to reduce API calls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the data type of contact email to string\n",
    "df['contact_email'] = df['contact_email'].astype(str)\n",
    "#split the email from @ and get the domain\n",
    "domain = df['contact_email'].str.split('@').str[1]\n",
    "# remove the dupicates to reduce the API calls\n",
    "domain.drop_duplicates(inplace=True)\n",
    "#reset the index\n",
    "domain = domain.reset_index(drop=True)\n",
    "#change the domain to a df which will be later used to merge\n",
    "df = pd.DataFrame(domain)\n",
    "#create a new column industry to store the result from the API\n",
    "df['industry'] = None\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to fetch the industry from the Enrich API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetchIndustry(df,df_merge,batchno,batchsize):\n",
    "    url = \"https://api.apollo.io/v1/organizations/enrich\"\n",
    "    headers = {\n",
    "        'Cache-Control': 'no-cache',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    #slice the df to smaller batch size, as 50 is allowed per minute\n",
    "    df = df.loc[batchsize*batchno:(batchsize*(batchno+1)-1),]\n",
    "    #iterate through the df, fetch the industry and store in the industry column\n",
    "    for i in range(batchsize*batchno,batchsize*(batchno+1)):\n",
    "        domain = df.loc[i,'contact_email'] \n",
    "        querystring = {\n",
    "            \"api_key\": \"Your API key\",\n",
    "            \"domain\": domain\n",
    "        }\n",
    "        try:\n",
    "            response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "            if response.status_code == 200: #if request is okay \n",
    "                industry = response.json()\n",
    "                if bool(response.json()) and bool(industry[\"organization\"][\"industry\"]):#if the response is not empty and industry is not an empty string\n",
    "                    # print(i)\n",
    "                    df.loc[i,'industry'] = industry[\"organization\"][\"industry\"]#save the industry to the df industry column\n",
    "                else:\n",
    "                   # print(i)\n",
    "                    df.loc[i,'industry'] = None\n",
    "            elif response.status_code == 429: #indicates that the API request weas exahusted\n",
    "                print(\"API requests exhausted\")\n",
    "            else:\n",
    "                print(response.text)\n",
    "        except Exception as error:\n",
    "        # Print the  error message\n",
    "            print(\"An error occurred:\", error)\n",
    "    #save the df to csv file\n",
    "    df_merge = pd.concat([df_merge, df])\n",
    "    df_merge.to_csv('TestCustidandIndustryInfo.csv',header=1,index=0)\n",
    "    return\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a loop that runs every hour for 3 times and 4 times every 2 min as we have the API restriction (600 request per day, 200 req per hour and 50 request every min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 20\n",
    "# Run  every hour for 3 times\n",
    "for i in range(3):\n",
    "    if \"df_merge\" not in locals():\n",
    "        df_merge = pd.DataFrame()\n",
    "        # batchno = int(len(df_merge)/batchsize)\n",
    "    else:\n",
    "        df_merge = pd.read_csv('CustidandIndustryInfo.csv')\n",
    "        # batchno = int(len(df_merge)/batchsize)\n",
    "    # Run the program every 2 minutes for 4 times\n",
    "    for j in range(4):\n",
    "        batchno = int(len(df_merge)/batchsize)\n",
    "        fetchIndustry(df,df_merge,batchno,batchsize) #run the function fetchIndustry\n",
    "        df_merge = pd.read_csv('CustidandIndustryInfo.csv')\n",
    "        time.sleep(120)  # Sleep for 2 minutes (120 seconds)\n",
    "    time.sleep(3600)  # Sleep for 1 hour (3600 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join the two df to have the industry for all the contact email's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id                           industry\n",
      "0     1055178                               None\n",
      "1     1037261                               None\n",
      "2     1047267                               None\n",
      "3     1050123                           printing\n",
      "4     1097555                               None\n",
      "...       ...                                ...\n",
      "3730  1021042                               None\n",
      "3731  1057037                               None\n",
      "3732  1090115                               None\n",
      "3733  1025342  information technology & services\n",
      "3734  1036675                               None\n",
      "\n",
      "[3701 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#get the industry domain df\n",
    "df_merge = pd.read_csv('CustidandIndustryInfo.csv')\n",
    "#change the data type of contact email to string\n",
    "df_backup['contact_email'] = df_backup['contact_email'].astype(str)\n",
    "#split the email from @ and get the domain\n",
    "df_backup['domain'] = df_backup['contact_email'].str.split('@').str[1]\n",
    "#join with the df with so that it can be inserted in DB with correct cust id\n",
    "df_insert = pd.merge(df_backup,df_merge,left_on='domain',right_on='contact_email',how=\"left\")\n",
    "# drop the unnecessary column \n",
    "df_insert = df_insert[['id','industry']]\n",
    "#drop the duplicates\n",
    "df_insert.drop_duplicates(inplace=True)\n",
    "#replace the NaN value to null\n",
    "df_insert = df_insert.where(pd.notnull(df_insert), None)\n",
    "print(df_insert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert the industry value to Customer table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    connection = connect_to_postgres() # get the connection\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # declare the table and column name\n",
    "    table_name = 'customers'\n",
    "    new_column_name = 'industry'\n",
    "    data_type = 'VARCHAR(99)'  # Adjust the data type based on your requirements\n",
    "\n",
    "    # Construct the SQL query to add the new column\n",
    "    query = f\"ALTER TABLE {table_name} ADD COLUMN {new_column_name} {data_type};\"\n",
    "\n",
    "    # Execute the query\n",
    "    cursor.execute(query)\n",
    "\n",
    "    # Use customer id to insert the industry \n",
    "    for index, row in df_insert.iterrows():\n",
    "        primary_key_value = row['id']\n",
    "        new_column_value = row['industry']\n",
    "\n",
    "        # Construct the SQL query\n",
    "        query = f\"UPDATE {table_name} SET industry = %s WHERE id = %s\"\n",
    "        values = (new_column_value, primary_key_value)\n",
    "        # Execute the query\n",
    "        cursor.execute(query, values)\n",
    "\n",
    "    # Commit the changes and close the connection\n",
    "    connection.commit()\n",
    "    \n",
    "    connection.close()\n",
    "except psycopg2.Error as error:\n",
    "    print(\"Error occured, Details:\", error)\n",
    "except Exception as error:\n",
    "    print(\"Error occured, Details:\", error)\n",
    "finally:\n",
    "    cursor.close()\n",
    "    connection.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
